{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29afee1c",
   "metadata": {},
   "source": [
    "# Correccion de Examen 2 \n",
    "\n",
    "### Jesús Emmanuel Flores Cortés "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66d49f",
   "metadata": {},
   "source": [
    "## Examen Teorico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b02b6",
   "metadata": {},
   "source": [
    "### 1.- Explica el modelo de regresión logística para clasificación. ¿Cómo se determina el umbral de decisión?\n",
    "\n",
    "La regresión logística sirve para clasificar observaciones en dos clases. Calcula la probabilidad de pertenecer a una clase usando una función sigmoide que transforma una combinación lineal de las variables en un valor entre 0 y 1.\n",
    "El umbral de decisión es el punto que separa las clases, normalmente 0.5. Si la probabilidad es mayor al umbral, se clasifica en una clase; si es menor, en la otra. El umbral puede ajustarse dependiendo del tipo de error que se quiera minimizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec4f15",
   "metadata": {},
   "source": [
    "### 2.-Explica la intuición de la máquina de soporte vectorial para clasificación. ¿Cómo se determina qué modelo es mejor? ¿Cuál es la mayor diferencia que tiene contra un modelo de regresión logística?\n",
    "\n",
    "La SVM busca una frontera que divida las clases de forma que la distancia entre los puntos más cercanos de cada clase y esa frontera sea máxima. Esa distancia se llama margen.\n",
    "Para decidir qué modelo es mejor, se comparan métricas como precisión, recall, F1 o exactitud en validación cruzada.\n",
    "La principal diferencia con la regresión logística es que la SVM no calcula probabilidades, sino que se enfoca en encontrar la frontera que mejor separa las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b5bbb",
   "metadata": {},
   "source": [
    "### 3.- ¿Cuáles son los componentes principales en un MLP para clasificación? Dibuja un ejemplo y señaliza.\n",
    "\n",
    "Un MLP está compuesto por:\n",
    "\n",
    "Una capa de entrada que recibe las variables\n",
    "\n",
    "Una o más capas ocultas donde las neuronas combinan las entradas aplicando pesos y funciones de activación\n",
    "\n",
    "Una capa de salida que entrega la predicción final.\n",
    "Cada conexión tiene un peso que se ajusta durante el entrenamiento para minimizar el error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379a95f",
   "metadata": {},
   "source": [
    "### 4.- ¿Cuál es el procedimiento a seguir cuando los datos no son linealmente separables en una SVC?\n",
    "\n",
    "Cuando los datos no son linealmente separables, se usa un kernel, que transforma los datos a un espacio de mayor dimensión donde sí pueden separarse. Algunos kernels comunes son lineal, polinomial y RBF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8760be2",
   "metadata": {},
   "source": [
    "### 5.- Describe qué es un hiperparámetro. ¿Por qué es importante ajustarlos? Da dos ejemplos de hiperparámetros.\n",
    "\n",
    "\n",
    "Los hiperparámetros son valores que definen el comportamiento del modelo antes del entrenamiento. No se aprenden de los datos, se eligen y ajustan para mejorar el rendimiento.\n",
    "Ejemplos: el parámetro C en una SVM (controla la penalización del error) y la tasa de aprendizaje en un MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce291a",
   "metadata": {},
   "source": [
    "### 6.- Dibuja un diagrama de flujo para describir el proceso de optimización Bayesiana.\n",
    "\n",
    "Definir el espacio de hiperparámetros\n",
    "\n",
    "Elegir un modelo probabilístico inicial \n",
    "\n",
    "Probar un conjunto de valores de hiperparámetros y medir el desempeño\n",
    "\n",
    "Actualizar el modelo probabilístico con los resultados\n",
    "\n",
    "Seleccionar los siguientes hiperparámetros a probar según la probabilidad de mejorar el resultado\n",
    "\n",
    "Repetir hasta encontrar los mejores valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf0660",
   "metadata": {},
   "source": [
    "### 7.- ¿Qué es la curva ROC y cómo se usa para evaluar el desempeño de un modelo?\n",
    "\n",
    "La curva ROC muestra la relación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) al variar el umbral de decisión.\n",
    "Se usa para evaluar el desempeño general de un modelo de clasificación. Un modelo es mejor mientras más se acerque la curva a la esquina superior izquierda. El área bajo la curva (AUC) resume su calidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db5214a",
   "metadata": {},
   "source": [
    "### 8.- Describe un espacio de Hilbert.\n",
    "\n",
    "Es un espacio matemático donde se pueden aplicar operaciones como suma, producto interno y medición de distancia. Es importante en aprendizaje automático porque permite representar datos en dimensiones altas y aplicar kernels de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e131fd",
   "metadata": {},
   "source": [
    "### 9.- ¿Qué significa que una función de costo sea convexa? ¿Qué beneficios hay de que un modelo tenga una función de costo convexa?\n",
    "\n",
    "Una función de costo convexa tiene una sola solución mínima global. Esto significa que el proceso de optimización no se queda atrapado en mínimos locales. Tener una función convexa garantiza que el modelo converge hacia la mejor solución posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4193e2",
   "metadata": {},
   "source": [
    "### 10.- Piensa en los 3 modelos aprendidos en este parcial: ¿En qué situaciones usarías cada uno y por qué?\n",
    "\n",
    "Regresión logística: cuando las clases son lineales y se buscan probabilidades interpretables.\n",
    "\n",
    "SVM: cuando los datos no son lineales y se necesita una separación clara entre clases.\n",
    "\n",
    "MLP: cuando la relación entre variables es compleja o no lineal, y se dispone de más datos y poder de cómputo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d6087",
   "metadata": {},
   "source": [
    "# Examen Practico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d75921",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de847c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d050952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion_objetivo(X):\n",
    "    x, y, z = X[:, 0], X[:, 1], X[:, 2]\n",
    "    \n",
    "    term_x = (6*x - 2)**2 * np.sin(12*x - 4)\n",
    "    term_y = (6*y - 2)**2 * np.cos(12*y - 4)\n",
    "    term_z = (6*z - 2)**2 * np.sin(12*z - 4)\n",
    "    \n",
    "    return term_x + term_y + term_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a013c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(X_candidatos, gpr, f_mejor):\n",
    "\n",
    "    mu, sigma = gpr.predict(X_candidatos, return_std=True)\n",
    "\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = (f_mejor - mu) / sigma\n",
    "        \n",
    "        ei = sigma * (Z * norm.cdf(Z) + norm.pdf(Z))\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "        \n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b447c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximo_punto_optimo(gpr, f_mejor, bounds, n_busqueda_aleatoria=10000):\n",
    "\n",
    "    X_candidatos = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_busqueda_aleatoria, bounds.shape[0]))\n",
    "    \n",
    "    ei_values = expected_improvement(X_candidatos, gpr, f_mejor)\n",
    "    \n",
    "    idx_max = np.argmax(ei_values)\n",
    "    \n",
    "    return X_candidatos[idx_max].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7b5c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Optimización Bayesiana (Total de evaluaciones: 20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = np.array([[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]])\n",
    "\n",
    "N_INITIAL = 5   \n",
    "N_ITERACIONES = 15 \n",
    "TOTAL_CALLS = N_INITIAL + N_ITERACIONES\n",
    "\n",
    "print(f\"Iniciando Optimización Bayesiana (Total de evaluaciones: {TOTAL_CALLS})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d3211ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicial (f_mejor): -3.336411 en [0.83244264 0.21233911 0.18182497]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) \n",
    "X_muestras = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(N_INITIAL, 3))\n",
    "Y_muestras = funcion_objetivo(X_muestras)\n",
    "\n",
    "f_mejor = np.min(Y_muestras)\n",
    "idx_mejor = np.argmin(Y_muestras)\n",
    "X_mejor = X_muestras[idx_mejor]\n",
    "\n",
    "print(f\"Inicial (f_mejor): {f_mejor:.6f} en {X_mejor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6046c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración  1: f(x) = -6.561923 | Mejor f(x) hasta ahora: -6.561923\n",
      "Iteración  2: f(x) = -6.646178 | Mejor f(x) hasta ahora: -6.646178\n",
      "Iteración  3: f(x) = -2.989764 | Mejor f(x) hasta ahora: -6.646178\n",
      "Iteración  4: f(x) = -6.210537 | Mejor f(x) hasta ahora: -6.646178\n",
      "Iteración  5: f(x) = -0.625627 | Mejor f(x) hasta ahora: -6.646178\n",
      "Iteración  6: f(x) = -2.633143 | Mejor f(x) hasta ahora: -6.646178\n",
      "Iteración  7: f(x) = -8.754775 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración  8: f(x) = -6.917209 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración  9: f(x) = -7.645515 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración 10: f(x) = -4.617412 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración 11: f(x) = -8.429537 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración 12: f(x) = -4.510327 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración 13: f(x) = -7.729734 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración 14: f(x) = -3.860670 | Mejor f(x) hasta ahora: -8.754775\n",
      "Iteración 15: f(x) = -5.211143 | Mejor f(x) hasta ahora: -8.754775\n",
      "Mínimo Global Estimado (f(x,y,z)): **-8.754775**\n",
      "Punto Óptimo (x, y, z): **(0.7413, 0.5694, 0.1580)**\n"
     ]
    }
   ],
   "source": [
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1e2))\n",
    "gpr = GPR(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=10, random_state=42)\n",
    "\n",
    "for i in range(N_ITERACIONES):\n",
    "    gpr.fit(X_muestras, Y_muestras)\n",
    "    \n",
    "    X_siguiente = proximo_punto_optimo(gpr, f_mejor, bounds)\n",
    "    \n",
    "    Y_siguiente = funcion_objetivo(X_siguiente)[0]\n",
    "    \n",
    "    X_muestras = np.vstack([X_muestras, X_siguiente])\n",
    "    Y_muestras = np.append(Y_muestras, Y_siguiente)\n",
    "    \n",
    "    if Y_siguiente < f_mejor:\n",
    "        f_mejor = Y_siguiente\n",
    "        X_mejor = X_siguiente[0]\n",
    "\n",
    "    print(f\"Iteración {i+1:2d}: f(x) = {Y_siguiente:.6f} | Mejor f(x) hasta ahora: {f_mejor:.6f}\")\n",
    "print(f\"Mínimo Global Estimado (f(x,y,z)): **{f_mejor:.6f}**\")\n",
    "print(f\"Punto Óptimo (x, y, z): **({X_mejor[0]:.4f}, {X_mejor[1]:.4f}, {X_mejor[2]:.4f})**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644596f7",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a13f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm, t\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "641ad347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos = pd.read_csv('adidas.csv')\n",
    "\n",
    "columnas_a_descartar = ['url', 'name', 'sku', 'description', 'images', 'source_website', \n",
    "                        'source', 'breadcrumbs', 'language', 'currency', 'color', 'crawled_at']\n",
    "\n",
    "df_productos = df_productos.drop(columns=columnas_a_descartar)\n",
    "\n",
    "df_productos['original_price'] = pd.to_numeric(df_productos['original_price'].astype(str).str.replace('$', '', regex=False), errors='coerce')\n",
    "\n",
    "df_productos['Y_clase_buena'] = (df_productos['average_rating'] >= 4.3).astype(int)\n",
    "\n",
    "lista_numericas = ['selling_price', 'original_price', 'reviews_count']\n",
    "lista_categoricas = ['availability', 'category', 'brand', 'country']\n",
    "df_modelado = df_productos.dropna().reset_index(drop=True)\n",
    "\n",
    "X_matriz = df_modelado.drop(columns=['average_rating', 'Y_clase_buena'])\n",
    "Y_vector = df_modelado['Y_clase_buena']\n",
    "\n",
    "X_matriz = pd.get_dummies(X_matriz, columns=lista_categoricas, drop_first=True)\n",
    "nombres_factores = X_matriz.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d694a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entreno, X_prueba, Y_entreno, Y_prueba = train_test_split(\n",
    "    X_matriz, Y_vector, train_size=0.7, random_state=42\n",
    ")\n",
    "escalador = StandardScaler().fit(X_entreno[lista_numericas])\n",
    "\n",
    "X_entreno.loc[:, lista_numericas] = escalador.transform(X_entreno[lista_numericas])\n",
    "X_prueba.loc[:, lista_numericas] = escalador.transform(X_prueba[lista_numericas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35a8577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_logistica = LogisticRegression(solver='liblinear', random_state=42)\n",
    "modelo_logistica.fit(X_entreno, Y_entreno)\n",
    "\n",
    "Y_prob_prueba = modelo_logistica.predict_proba(X_prueba)[:, 1]\n",
    "auc_final = roc_auc_score(Y_prueba, Y_prob_prueba)\n",
    "\n",
    "Y_prob_entreno = modelo_logistica.predict_proba(X_entreno)[:, 1]\n",
    "auc_entreno = roc_auc_score(Y_entreno, Y_prob_entreno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3ddcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = X_entreno.shape[0]\n",
    "P = X_entreno.shape[1] \n",
    "\n",
    "RSS = np.sum((Y_prob_prueba - Y_prueba) ** 2)\n",
    "\n",
    "varianza_residual = RSS / (N - P - 1)\n",
    "\n",
    "beta_intercepto = np.ravel(modelo_logistica.intercept_)\n",
    "beta_coeficientes = np.ravel(modelo_logistica.coef_)\n",
    "\n",
    "X_con_intercepto = np.column_stack((np.ones(N), X_entreno.values)).astype(float)\n",
    "\n",
    "inversa_X_t_X = np.linalg.inv(X_con_intercepto.T @ X_con_intercepto + np.eye(X_con_intercepto.shape[1]) * 1e-6)\n",
    "matriz_covarianza_beta = inversa_X_t_X * varianza_residual\n",
    "\n",
    "errores_estandar = np.sqrt(np.diag(matriz_covarianza_beta))\n",
    "\n",
    "vector_betas = np.concatenate((beta_intercepto, beta_coeficientes))\n",
    "\n",
    "estadisticos_t = vector_betas / errores_estandar\n",
    "\n",
    "p_valores = [2 * (1 - t.cdf(np.abs(stat), N - P - 1)) for stat in estadisticos_t]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7bba9767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Clasificación (AUC):\n",
      "  AUC Entrenamiento: 0.6750\n",
      "  AUC Prueba:        0.7643\n"
     ]
    }
   ],
   "source": [
    "print(f\"Métricas de Clasificación (AUC):\")\n",
    "print(f\"  AUC Entrenamiento: {auc_entreno:.4f}\")\n",
    "print(f\"  AUC Prueba:        {auc_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84b0d8",
   "metadata": {},
   "source": [
    "El AUC de 0.7643 en prueba sugiere que el modelo tiene capacidad de distinguir entre clases, con una precisión de 76.43% sobre el azar.\n",
    "Todos los factores muestran P-Values muy bajos, indicando que son estadísticamente significativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdedede3",
   "metadata": {},
   "source": [
    "# Pregunta 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "007b3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be2706c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d8df4b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.639947</td>\n",
       "      <td>0.848324</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>1.425995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-1.123396</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.530902</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>-0.365061</td>\n",
       "      <td>-0.190672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>1.943724</td>\n",
       "      <td>-0.263941</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-1.103255</td>\n",
       "      <td>0.604397</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.998208</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.123302</td>\n",
       "      <td>-0.494043</td>\n",
       "      <td>-0.920763</td>\n",
       "      <td>-1.041549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>0.504055</td>\n",
       "      <td>-1.504687</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>0.765836</td>\n",
       "      <td>1.409746</td>\n",
       "      <td>5.484909</td>\n",
       "      <td>-0.020496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>-0.622642</td>\n",
       "      <td>0.356432</td>\n",
       "      <td>1.722735</td>\n",
       "      <td>0.870031</td>\n",
       "      <td>0.115169</td>\n",
       "      <td>-0.908682</td>\n",
       "      <td>2.532136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.547919</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.405445</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.610154</td>\n",
       "      <td>-0.398282</td>\n",
       "      <td>-0.531023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.342981</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.279594</td>\n",
       "      <td>-0.735190</td>\n",
       "      <td>-0.685193</td>\n",
       "      <td>-0.275760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.159787</td>\n",
       "      <td>-0.470732</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.240205</td>\n",
       "      <td>-0.371101</td>\n",
       "      <td>1.170732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.873019</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.656358</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.202129</td>\n",
       "      <td>-0.473785</td>\n",
       "      <td>-0.871374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.639947  0.848324       0.149641       0.907270 -0.692891  0.204013   \n",
       "1      -0.844885 -1.123396      -0.160546       0.530902 -0.692891 -0.684422   \n",
       "2       1.233880  1.943724      -0.263941      -1.288212 -0.692891 -1.103255   \n",
       "3      -0.844885 -0.998208      -0.160546       0.154533  0.123302 -0.494043   \n",
       "4      -1.141852  0.504055      -1.504687       0.907270  0.765836  1.409746   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "763     1.827813 -0.622642       0.356432       1.722735  0.870031  0.115169   \n",
       "764    -0.547919  0.034598       0.046245       0.405445 -0.692891  0.610154   \n",
       "765     0.342981  0.003301       0.149641       0.154533  0.279594 -0.735190   \n",
       "766    -0.844885  0.159787      -0.470732      -1.288212 -0.692891 -0.240205   \n",
       "767    -0.844885 -0.873019       0.046245       0.656358 -0.692891 -0.202129   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  \n",
       "0                    0.468492  1.425995  \n",
       "1                   -0.365061 -0.190672  \n",
       "2                    0.604397 -0.105584  \n",
       "3                   -0.920763 -1.041549  \n",
       "4                    5.484909 -0.020496  \n",
       "..                        ...       ...  \n",
       "763                 -0.908682  2.532136  \n",
       "764                 -0.398282 -0.531023  \n",
       "765                 -0.685193 -0.275760  \n",
       "766                 -0.371101  1.170732  \n",
       "767                 -0.473785 -0.871374  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Outcome']\n",
    "X = df.drop(columns=['Outcome'])\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2534551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression: Precision = 0.7332 ± 0.1258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr_results': {'mean': 0.7331637099284157,\n",
       "  'std': 0.12577367742882836,\n",
       "  'predictions': array([0.71981638, 0.04933123, 0.79421539, 0.04226898, 0.90041553,\n",
       "         0.14753243, 0.06731905, 0.64097778, 0.7097593 , 0.03721347,\n",
       "         0.22044378, 0.89587106, 0.78299976, 0.63136585, 0.62706761,\n",
       "         0.39906913, 0.37177202, 0.19736995, 0.35667009, 0.23497608,\n",
       "         0.39290264, 0.31760973, 0.93916912, 0.29478265, 0.70111706,\n",
       "         0.44118653, 0.7353875 , 0.04651918, 0.5392435 , 0.27861723,\n",
       "         0.42608398, 0.57257665, 0.04967327, 0.03712914, 0.43217669,\n",
       "         0.15065195, 0.66342646, 0.39336248, 0.17298955, 0.5723509 ,\n",
       "         0.74155082, 0.69469157, 0.11492119, 0.92606855, 0.62715685,\n",
       "         0.95124578, 0.43321492, 0.04038956, 0.38030842, 0.03940934,\n",
       "         0.03682523, 0.08594148, 0.06815446, 0.82610491, 0.70780091,\n",
       "         0.02307931, 0.8817425 , 0.35816004, 0.83150367, 0.18399248,\n",
       "         0.01011081, 0.52045455, 0.02385955, 0.3085604 , 0.35427064,\n",
       "         0.11950116, 0.1934116 , 0.47113114, 0.03254582, 0.30235727,\n",
       "         0.19435476, 0.36412302, 0.81553638, 0.2473451 , 0.05229664,\n",
       "         0.00203153, 0.07719661, 0.22264235, 0.66849353, 0.10099177,\n",
       "         0.10304983, 0.00601279, 0.13816207, 0.05071594, 0.67424721,\n",
       "         0.19287928, 0.52880672, 0.1877996 , 0.78310101, 0.07509597,\n",
       "         0.01963785, 0.25928607, 0.33143355, 0.29688766, 0.25522516,\n",
       "         0.51549517, 0.08263986, 0.01765711, 0.134758  , 0.45204085,\n",
       "         0.83485704, 0.28223551, 0.06336086, 0.03234252, 0.23167185,\n",
       "         0.2473727 , 0.01992434, 0.42494587, 0.10747755, 0.09396762,\n",
       "         0.59968385, 0.70680427, 0.05470041, 0.09417003, 0.73860716,\n",
       "         0.55332091, 0.35675798, 0.15874376, 0.11886339, 0.05344606,\n",
       "         0.88466872, 0.28481945, 0.14988301, 0.35468975, 0.13665439,\n",
       "         0.52528797, 0.45764913, 0.19556077, 0.19544672, 0.15569068,\n",
       "         0.63160321, 0.68322478, 0.67171   , 0.2728204 , 0.05564973,\n",
       "         0.24322143, 0.09848338, 0.0685107 , 0.26202433, 0.17713753,\n",
       "         0.16850189, 0.32005654, 0.17295495, 0.38572039, 0.46655651,\n",
       "         0.00725049, 0.0644852 , 0.30267378, 0.63088768, 0.04550641,\n",
       "         0.35298459, 0.13753085, 0.84450412, 0.52286488, 0.9605947 ,\n",
       "         0.87861362, 0.0878376 , 0.12534199, 0.05023458, 0.96973691,\n",
       "         0.4255449 , 0.30219561, 0.22335549, 0.10457051, 0.27835554,\n",
       "         0.23416478, 0.44635538, 0.31755737, 0.23481232, 0.12385192,\n",
       "         0.15543692, 0.52232081, 0.19903154, 0.19072772, 0.05203983,\n",
       "         0.87217757, 0.12802155, 0.77515694, 0.75158936, 0.65594286,\n",
       "         0.04668114, 0.26687572, 0.00210051, 0.04989806, 0.34622475,\n",
       "         0.94790489, 0.8381326 , 0.37372773, 0.24316848, 0.35765151,\n",
       "         0.07778729, 0.48210011, 0.68668627, 0.97054858, 0.10004493,\n",
       "         0.68066726, 0.06411867, 0.10903359, 0.34112956, 0.37279491,\n",
       "         0.17343958, 0.42891336, 0.13909586, 0.03993994, 0.33295461,\n",
       "         0.13139801, 0.9485112 , 0.69366492, 0.0935751 , 0.87469974,\n",
       "         0.04971967, 0.55210567, 0.83368662, 0.52359534, 0.31360584,\n",
       "         0.89612807, 0.30165369, 0.33357874, 0.17797923, 0.38184963,\n",
       "         0.70966186, 0.6845619 , 0.41171032, 0.64226956, 0.07213422,\n",
       "         0.06055729, 0.11116644, 0.78329884, 0.95943339, 0.28852171,\n",
       "         0.69197801, 0.64966186, 0.03165361, 0.35779435, 0.04414077,\n",
       "         0.86690448, 0.88185956, 0.83721236, 0.79842189, 0.0415299 ,\n",
       "         0.05769569, 0.1190059 , 0.29648809, 0.45601879, 0.48057117,\n",
       "         0.93083847, 0.46561079, 0.70029198, 0.40791157, 0.08926672,\n",
       "         0.38245739, 0.17484976, 0.03507368, 0.07964614, 0.30874826,\n",
       "         0.21462646, 0.23522411, 0.11918239, 0.66743954, 0.90364617,\n",
       "         0.7649093 , 0.67219999, 0.15933609, 0.48423101, 0.3086462 ,\n",
       "         0.29619351, 0.71890697, 0.62045913, 0.0541583 , 0.51837171,\n",
       "         0.72639066, 0.07393632, 0.13308356, 0.04148721, 0.51731559,\n",
       "         0.2761123 , 0.18148926, 0.07974711, 0.27434203, 0.11225999,\n",
       "         0.48109716, 0.57088995, 0.37665923, 0.6345324 , 0.1222766 ,\n",
       "         0.45399772, 0.61943541, 0.45187015, 0.0605434 , 0.26288321,\n",
       "         0.05651653, 0.23795874, 0.64729101, 0.48542434, 0.44126202,\n",
       "         0.71889492, 0.25145161, 0.15229696, 0.48827848, 0.34073794,\n",
       "         0.82945887, 0.40035225, 0.09073991, 0.57548713, 0.24282347,\n",
       "         0.2985355 , 0.67477984, 0.12155494, 0.35731975, 0.3313852 ,\n",
       "         0.07996272, 0.21015135, 0.35456675, 0.22974372, 0.54156022,\n",
       "         0.18631143, 0.03835173, 0.70307281, 0.26245786, 0.77166057,\n",
       "         0.26697087, 0.15909884, 0.1515744 , 0.74187652, 0.17654048,\n",
       "         0.24386634, 0.31903943, 0.88684143, 0.21621035, 0.17798568,\n",
       "         0.48350072, 0.07944614, 0.94151566, 0.20510371, 0.04457196,\n",
       "         0.73648954, 0.56621477, 0.27408086, 0.77224334, 0.88928648,\n",
       "         0.16045169, 0.07661172, 0.00380806, 0.31789078, 0.39495484,\n",
       "         0.56104475, 0.3597763 , 0.20880196, 0.05549326, 0.01404014,\n",
       "         0.21481205, 0.32562764, 0.04996573, 0.06737391, 0.22535301,\n",
       "         0.74426603, 0.39785937, 0.92380795, 0.35075476, 0.86336075,\n",
       "         0.80370605, 0.65420746, 0.30989155, 0.75392789, 0.47022011,\n",
       "         0.24281229, 0.26770447, 0.03753524, 0.03566665, 0.21971532,\n",
       "         0.90523335, 0.0379946 , 0.0895923 , 0.16729733, 0.4280684 ,\n",
       "         0.80516864, 0.03729025, 0.12620786, 0.83099204, 0.24214405,\n",
       "         0.16064804, 0.03757054, 0.12318199, 0.10258963, 0.10320911,\n",
       "         0.09060344, 0.3464481 , 0.43649763, 0.50662165, 0.20547541,\n",
       "         0.12672194, 0.8630086 , 0.10066943, 0.12952441, 0.68299703,\n",
       "         0.39972633, 0.13917007, 0.25672762, 0.0313092 , 0.8250677 ,\n",
       "         0.12663452, 0.37147629, 0.43198074, 0.10805882, 0.72010149,\n",
       "         0.50012779, 0.23366822, 0.04701305, 0.92632233, 0.7469712 ,\n",
       "         0.27011652, 0.17831417, 0.60325174, 0.19970446, 0.35254567,\n",
       "         0.54029044, 0.12421393, 0.64557771, 0.02600515, 0.18664197,\n",
       "         0.37419355, 0.06605587, 0.21270865, 0.18593268, 0.81498349,\n",
       "         0.77470909, 0.01137648, 0.72336605, 0.32025208, 0.09844159,\n",
       "         0.09639969, 0.10385952, 0.04810418, 0.20132188, 0.10012083,\n",
       "         0.72373609, 0.74467672, 0.47172494, 0.02437813, 0.3489509 ,\n",
       "         0.72364981, 0.0782304 , 0.22800296, 0.39536998, 0.24162973,\n",
       "         0.99227423, 0.077457  , 0.09743011, 0.13907017, 0.13785157,\n",
       "         0.02419162, 0.28277959, 0.11738418, 0.41125576, 0.2231845 ,\n",
       "         0.91862449, 0.42927875, 0.08982488, 0.84937016, 0.58553928,\n",
       "         0.30792714, 0.02077461, 0.20377639, 0.08090053, 0.30912897,\n",
       "         0.09960163, 0.03110072, 0.15504343, 0.55434007, 0.83638241,\n",
       "         0.59916379, 0.26315834, 0.26311914, 0.41800246, 0.15692829,\n",
       "         0.24328256, 0.16879791, 0.16521979, 0.2732422 , 0.366948  ,\n",
       "         0.56140798, 0.19044028, 0.07391535, 0.06491397, 0.84448312,\n",
       "         0.41623295, 0.42744514, 0.92140521, 0.08500758, 0.89875197,\n",
       "         0.12945904, 0.0921466 , 0.14989689, 0.44093918, 0.00893811,\n",
       "         0.69797976, 0.14632363, 0.06035082, 0.80928708, 0.63468906,\n",
       "         0.08073233, 0.11859071, 0.02349765, 0.29544302, 0.18235412,\n",
       "         0.13997155, 0.67276313, 0.23843908, 0.12080088, 0.34302191,\n",
       "         0.21819817, 0.11107322, 0.14605343, 0.0758406 , 0.07227274,\n",
       "         0.53752468, 0.66586609, 0.5315326 , 0.23102271, 0.18379986,\n",
       "         0.01901339, 0.22876015, 0.04391115, 0.66593229, 0.25408915,\n",
       "         0.04472622, 0.02762877, 0.0973013 , 0.14083007, 0.11253535,\n",
       "         0.24945756, 0.35748017, 0.23590025, 0.29678775, 0.13774768,\n",
       "         0.57288805, 0.08564021, 0.02835283, 0.29558382, 0.44504301,\n",
       "         0.42137043, 0.29477537, 0.40779381, 0.10605402, 0.06687558,\n",
       "         0.84698458, 0.96092004, 0.298337  , 0.58573595, 0.73701329,\n",
       "         0.10472767, 0.09193381, 0.24822235, 0.06820592, 0.10720597,\n",
       "         0.20586557, 0.15425276, 0.27395951, 0.63906965, 0.17739115,\n",
       "         0.4182204 , 0.87577309, 0.11106434, 0.15849191, 0.08463866,\n",
       "         0.09559988, 0.16900017, 0.14935309, 0.52195324, 0.19171993,\n",
       "         0.07877369, 0.09451184, 0.17458978, 0.11843595, 0.29331856,\n",
       "         0.2871105 , 0.24054884, 0.43483521, 0.44654065, 0.92110364,\n",
       "         0.54179068, 0.14553483, 0.46184743, 0.3460226 , 0.31944687,\n",
       "         0.04759398, 0.64228427, 0.11242164, 0.84321642, 0.03774293,\n",
       "         0.81595368, 0.20885012, 0.41645957, 0.1751146 , 0.41814583,\n",
       "         0.6765681 , 0.10756277, 0.10433367, 0.68076703, 0.09682487,\n",
       "         0.07838974, 0.17248811, 0.13684235, 0.76317846, 0.85609996,\n",
       "         0.33317521, 0.85967413, 0.03520968, 0.48214083, 0.05893609,\n",
       "         0.14319915, 0.75608263, 0.83135984, 0.29156972, 0.76203068,\n",
       "         0.08762684, 0.16098145, 0.01490924, 0.51255059, 0.30360846,\n",
       "         0.19350245, 0.1515971 , 0.96104129, 0.16761202, 0.12121554,\n",
       "         0.14315229, 0.10132028, 0.22999418, 0.39129431, 0.05853674,\n",
       "         0.32533777, 0.09530925, 0.11612212, 0.10309374, 0.1402538 ,\n",
       "         0.42501373, 0.15921352, 0.10610489, 0.43421507, 0.0301739 ,\n",
       "         0.08506309, 0.34881315, 0.49908244, 0.23146255, 0.1239708 ,\n",
       "         0.49629571, 0.36812279, 0.76679607, 0.47287135, 0.0705292 ,\n",
       "         0.04460114, 0.23143409, 0.31326467, 0.19492315, 0.11010578,\n",
       "         0.51140125, 0.04653966, 0.46614646, 0.60836036, 0.15732647,\n",
       "         0.70266074, 0.96062283, 0.72327487, 0.76831425, 0.36634054,\n",
       "         0.13655566, 0.57098539, 0.27162706, 0.25080149, 0.63923497,\n",
       "         0.79862399, 0.07983625, 0.11232056, 0.7283863 , 0.36724572,\n",
       "         0.85628646, 0.56804996, 0.10548777, 0.32002845, 0.07081678,\n",
       "         0.01572825, 0.81235435, 0.20758368, 0.31546835, 0.07981031,\n",
       "         0.28454468, 0.16592901, 0.11664159, 0.22290491, 0.65413799,\n",
       "         0.23249262, 0.87150799, 0.4390656 , 0.61610772, 0.04245049,\n",
       "         0.30947559, 0.54442445, 0.10793766, 0.32859636, 0.62926354,\n",
       "         0.26263355, 0.36635249, 0.77422087, 0.67125486, 0.11006511,\n",
       "         0.15004582, 0.08097721, 0.24859282, 0.76384614, 0.18000749,\n",
       "         0.41428188, 0.32343765, 0.77275405, 0.14488828, 0.10106597,\n",
       "         0.90322656, 0.7709679 , 0.2078938 , 0.17858496, 0.2607099 ,\n",
       "         0.06436799, 0.20450064, 0.37865615, 0.365225  , 0.14742003,\n",
       "         0.33988409, 0.20573142, 0.28801066, 0.38045292, 0.07977737,\n",
       "         0.22845055, 0.22778409, 0.83387108, 0.11759936, 0.11486517,\n",
       "         0.18447613, 0.12163507, 0.1187914 , 0.16448267, 0.22641239,\n",
       "         0.76717464, 0.16782463, 0.09565869, 0.66091737, 0.92924191,\n",
       "         0.31217664, 0.69281453, 0.31132845, 0.82614966, 0.57279668,\n",
       "         0.54352397, 0.27592134, 0.10670918, 0.66567141, 0.72366832,\n",
       "         0.44174397, 0.48015936, 0.31866953, 0.16483335, 0.90090307,\n",
       "         0.09695624, 0.93467837, 0.08814316, 0.31900435, 0.31859379,\n",
       "         0.1714758 , 0.28563864, 0.0727153 ])}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model(x: np.ndarray, y: np.ndarray, C: float) -> dict:\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=1000, C=C)\n",
    "    lr_model.fit(x, y)\n",
    "    lr_scores = cross_val_score(lr_model, x, y, cv=kf, scoring='precision')\n",
    "    lr_predict = lr_model.predict_proba(x)[:, 1]\n",
    "    lr_results = {\n",
    "        'mean': lr_scores.mean(),\n",
    "        'std': lr_scores.std(),\n",
    "        'predictions': lr_predict\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"\\nLogistic Regression: Precision = {lr_results['mean']:.4f} ± {lr_results['std']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'lr_results': lr_results\n",
    "    }\n",
    "\n",
    "run_model(X.values, y.values, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3222791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\n",
      "linear: precision = 0.7305 ± 0.0909\n",
      "rbf: precision = 0.7143 ± 0.1083\n",
      "poly: precision = 0.7520 ± 0.0957\n",
      "\n",
      "Logistic Regression: precision = 0.7327 ± 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svm_results': {'linear': {'mean': 0.7304647981585983,\n",
       "   'std': 0.09093566725853251},\n",
       "  'rbf': {'mean': 0.7142564635127564, 'std': 0.10834723244016789},\n",
       "  'poly': {'mean': 0.7519735330029448, 'std': 0.09572202338107426}},\n",
       " 'lr_results': {'mean': 0.7327120705381575,\n",
       "  'std': 0.10500195002597894,\n",
       "  'predictions': array([0.71788557, 0.05004217, 0.7916649 , 0.04292874, 0.89864691,\n",
       "         0.14842183, 0.06808818, 0.63750014, 0.70997257, 0.03807967,\n",
       "         0.22122976, 0.89393557, 0.78146623, 0.6337848 , 0.62645407,\n",
       "         0.39738602, 0.37235677, 0.19810911, 0.35598437, 0.23576997,\n",
       "         0.39330902, 0.3185104 , 0.93786761, 0.29480507, 0.70010361,\n",
       "         0.44101334, 0.73329128, 0.04727481, 0.53929577, 0.27939413,\n",
       "         0.42647917, 0.5716599 , 0.05034582, 0.03780712, 0.43187454,\n",
       "         0.15193868, 0.66127902, 0.39334987, 0.1735713 , 0.57246524,\n",
       "         0.73887433, 0.6926571 , 0.11627473, 0.92515896, 0.62521318,\n",
       "         0.94990929, 0.43202883, 0.04098976, 0.37956448, 0.03986138,\n",
       "         0.03749065, 0.08672076, 0.06897989, 0.82526127, 0.70684618,\n",
       "         0.02349474, 0.88029203, 0.35837466, 0.82964008, 0.18471886,\n",
       "         0.01032847, 0.51924463, 0.02437444, 0.30859252, 0.35433509,\n",
       "         0.12041188, 0.19449076, 0.471383  , 0.03312359, 0.30275219,\n",
       "         0.19521642, 0.36390009, 0.81352824, 0.24856769, 0.05297115,\n",
       "         0.00210559, 0.07824708, 0.22300333, 0.66486304, 0.10174511,\n",
       "         0.10364212, 0.00616639, 0.13937028, 0.05135108, 0.67255807,\n",
       "         0.19385721, 0.52750281, 0.18851854, 0.78109638, 0.07584466,\n",
       "         0.02002706, 0.26047819, 0.33183473, 0.29813446, 0.25565044,\n",
       "         0.51530937, 0.08335021, 0.0180366 , 0.13542119, 0.45204381,\n",
       "         0.83257741, 0.28198402, 0.06418534, 0.03292025, 0.23208479,\n",
       "         0.24780365, 0.02042406, 0.42466657, 0.108202  , 0.09487402,\n",
       "         0.59775931, 0.70659129, 0.05542445, 0.0949544 , 0.73665918,\n",
       "         0.55325455, 0.35675488, 0.15928957, 0.11951508, 0.0541797 ,\n",
       "         0.88258148, 0.28450174, 0.15078459, 0.35608822, 0.13743818,\n",
       "         0.52305771, 0.45697679, 0.19609191, 0.19697618, 0.15746995,\n",
       "         0.63012116, 0.68074757, 0.66994572, 0.27331335, 0.05643361,\n",
       "         0.24395448, 0.09930611, 0.06930825, 0.26248807, 0.17866197,\n",
       "         0.1700225 , 0.32029731, 0.17346228, 0.38554744, 0.46604519,\n",
       "         0.00745898, 0.06541007, 0.30326581, 0.63018036, 0.04613325,\n",
       "         0.35311426, 0.13854645, 0.84290125, 0.5232295 , 0.95957638,\n",
       "         0.87656118, 0.08864848, 0.12628821, 0.05094959, 0.96894917,\n",
       "         0.42502866, 0.30307878, 0.22471488, 0.1052561 , 0.27893459,\n",
       "         0.23556719, 0.44472541, 0.3176704 , 0.23523254, 0.12503067,\n",
       "         0.15650991, 0.52115783, 0.19881975, 0.19118321, 0.05283464,\n",
       "         0.87028685, 0.12922758, 0.77310705, 0.7496119 , 0.6540884 ,\n",
       "         0.04743989, 0.26716085, 0.00218093, 0.05058131, 0.34639707,\n",
       "         0.94663077, 0.83761892, 0.37400458, 0.24399292, 0.35774735,\n",
       "         0.07849914, 0.48141979, 0.68436039, 0.96955705, 0.10101736,\n",
       "         0.6790186 , 0.06479299, 0.10986614, 0.34093155, 0.37335504,\n",
       "         0.17409539, 0.42805683, 0.1400063 , 0.0406062 , 0.33444317,\n",
       "         0.13219342, 0.94767339, 0.69242281, 0.09440086, 0.87272924,\n",
       "         0.05036594, 0.54997205, 0.83210529, 0.52209049, 0.31450793,\n",
       "         0.89465487, 0.30175257, 0.33382327, 0.17886049, 0.3816682 ,\n",
       "         0.70848132, 0.68383451, 0.41010137, 0.64180189, 0.07299386,\n",
       "         0.06130329, 0.1120406 , 0.78021311, 0.95887782, 0.28860256,\n",
       "         0.68939321, 0.64948089, 0.03222971, 0.35734703, 0.04479347,\n",
       "         0.8645643 , 0.88054969, 0.83472464, 0.79602816, 0.04221562,\n",
       "         0.05834496, 0.11987824, 0.29606923, 0.45556587, 0.48002338,\n",
       "         0.92952842, 0.46482273, 0.70031005, 0.40895783, 0.0901078 ,\n",
       "         0.38211727, 0.17568269, 0.03570694, 0.08039607, 0.3102506 ,\n",
       "         0.21479135, 0.23543343, 0.11992521, 0.66603805, 0.90231929,\n",
       "         0.76279015, 0.66868744, 0.15995989, 0.4844348 , 0.30870513,\n",
       "         0.29709797, 0.71530487, 0.61778994, 0.05477814, 0.51553943,\n",
       "         0.7242176 , 0.07467145, 0.13425142, 0.04211877, 0.51666327,\n",
       "         0.27619859, 0.18204183, 0.08064143, 0.27566703, 0.11352744,\n",
       "         0.47967141, 0.56981978, 0.37729929, 0.6331524 , 0.12368868,\n",
       "         0.45443118, 0.61984003, 0.45186674, 0.06129164, 0.26341363,\n",
       "         0.05728762, 0.23836485, 0.64583118, 0.48415882, 0.44149582,\n",
       "         0.71650579, 0.25294942, 0.15360111, 0.48845654, 0.34173925,\n",
       "         0.82638732, 0.3997098 , 0.09176508, 0.57379466, 0.24356536,\n",
       "         0.29888121, 0.67340186, 0.12254169, 0.35757595, 0.33199771,\n",
       "         0.08092507, 0.21090116, 0.35451315, 0.23015492, 0.54050007,\n",
       "         0.18711569, 0.03906771, 0.70054525, 0.26298064, 0.77005482,\n",
       "         0.26800079, 0.15975306, 0.15252191, 0.73998173, 0.17700936,\n",
       "         0.24461568, 0.31942736, 0.88483718, 0.21698502, 0.17900935,\n",
       "         0.4830015 , 0.08024003, 0.93991312, 0.20624001, 0.04521853,\n",
       "         0.73456735, 0.56390722, 0.2748989 , 0.77032133, 0.88747485,\n",
       "         0.16133965, 0.07766714, 0.00393297, 0.31814279, 0.39549091,\n",
       "         0.56058935, 0.35902642, 0.20846484, 0.05626024, 0.01440813,\n",
       "         0.21546203, 0.32568076, 0.05085543, 0.06815834, 0.22571015,\n",
       "         0.74252089, 0.39744159, 0.92190907, 0.35126021, 0.86160927,\n",
       "         0.80194813, 0.65315785, 0.31158685, 0.75267578, 0.47028515,\n",
       "         0.24325792, 0.26797602, 0.03809606, 0.0363126 , 0.22146246,\n",
       "         0.90407143, 0.0386588 , 0.09040442, 0.16799435, 0.42736251,\n",
       "         0.80436346, 0.0379578 , 0.12696744, 0.8285916 , 0.24329457,\n",
       "         0.16152461, 0.03814398, 0.12425132, 0.10350625, 0.10418802,\n",
       "         0.0913855 , 0.34633481, 0.43642346, 0.50779375, 0.20628267,\n",
       "         0.12820076, 0.86067828, 0.10209908, 0.13076001, 0.68072105,\n",
       "         0.3999619 , 0.14037828, 0.25658665, 0.03186851, 0.82233809,\n",
       "         0.12750189, 0.37189261, 0.43165492, 0.10910621, 0.71781179,\n",
       "         0.49875099, 0.23471203, 0.04767558, 0.924815  , 0.74619269,\n",
       "         0.27044185, 0.17938729, 0.60237747, 0.20027336, 0.35228437,\n",
       "         0.54043702, 0.12496493, 0.64356999, 0.02651953, 0.18749635,\n",
       "         0.37436311, 0.0668561 , 0.2130853 , 0.1863063 , 0.81320086,\n",
       "         0.77298836, 0.0116194 , 0.72155768, 0.32072517, 0.09988862,\n",
       "         0.09679414, 0.10507026, 0.04882894, 0.2020011 , 0.10116216,\n",
       "         0.72017613, 0.74246505, 0.47046389, 0.02483454, 0.34899055,\n",
       "         0.72182404, 0.07902352, 0.22855514, 0.39491495, 0.24194183,\n",
       "         0.99195052, 0.07841073, 0.09840796, 0.13980772, 0.13878422,\n",
       "         0.02469334, 0.28273441, 0.11853812, 0.411318  , 0.22362876,\n",
       "         0.91685459, 0.42958729, 0.09068894, 0.8480947 , 0.58593667,\n",
       "         0.30883988, 0.02120726, 0.20474214, 0.08186741, 0.3097514 ,\n",
       "         0.10045673, 0.03161769, 0.15589712, 0.55169126, 0.83418704,\n",
       "         0.59698516, 0.26302389, 0.26298128, 0.41832269, 0.1575547 ,\n",
       "         0.24475949, 0.17008318, 0.16636626, 0.27396428, 0.36800768,\n",
       "         0.56111002, 0.19118238, 0.07473025, 0.06581151, 0.84151805,\n",
       "         0.41604242, 0.42803789, 0.92043315, 0.08585348, 0.89744342,\n",
       "         0.13029798, 0.09329103, 0.15069441, 0.44119846, 0.00913969,\n",
       "         0.69687925, 0.14717442, 0.06118458, 0.80775213, 0.63359818,\n",
       "         0.08166272, 0.11940288, 0.02401845, 0.29611762, 0.18332219,\n",
       "         0.14110599, 0.67105154, 0.2390411 , 0.12155106, 0.34427613,\n",
       "         0.21916825, 0.11213769, 0.14753096, 0.0765799 , 0.07306256,\n",
       "         0.53608877, 0.66521794, 0.53101472, 0.23172691, 0.18650435,\n",
       "         0.01943698, 0.2295789 , 0.0443903 , 0.66413293, 0.25408009,\n",
       "         0.04532329, 0.02815104, 0.09827642, 0.14187848, 0.11346767,\n",
       "         0.24982197, 0.35698648, 0.23639805, 0.29597873, 0.13852716,\n",
       "         0.56948907, 0.08686221, 0.02908608, 0.29628171, 0.44500312,\n",
       "         0.42195383, 0.29545435, 0.40829841, 0.10704555, 0.06778773,\n",
       "         0.84538846, 0.96012739, 0.2988417 , 0.58486893, 0.73508714,\n",
       "         0.10543267, 0.0928822 , 0.25005667, 0.06895933, 0.1082223 ,\n",
       "         0.20725461, 0.15498428, 0.27526807, 0.63695629, 0.17831446,\n",
       "         0.41847796, 0.87400963, 0.11193899, 0.15938936, 0.08555337,\n",
       "         0.09644026, 0.16955736, 0.15071601, 0.5211194 , 0.19285608,\n",
       "         0.0797655 , 0.09542532, 0.17509067, 0.11932   , 0.29462825,\n",
       "         0.2867729 , 0.24129644, 0.43365537, 0.44567668, 0.91965408,\n",
       "         0.5397091 , 0.14617963, 0.46211464, 0.34626965, 0.32272394,\n",
       "         0.04820872, 0.64029577, 0.11328811, 0.84186849, 0.03819435,\n",
       "         0.81388451, 0.20963168, 0.41640545, 0.17598216, 0.41839473,\n",
       "         0.67471392, 0.10878339, 0.10484307, 0.6786186 , 0.09766786,\n",
       "         0.0792493 , 0.17256571, 0.13766505, 0.76180736, 0.85332034,\n",
       "         0.33236534, 0.85788622, 0.03580073, 0.48192657, 0.05980755,\n",
       "         0.14412674, 0.75415463, 0.83008029, 0.29162014, 0.76052893,\n",
       "         0.08848572, 0.16201892, 0.01524906, 0.51211327, 0.30243309,\n",
       "         0.19463719, 0.15253119, 0.96012132, 0.16841326, 0.1218988 ,\n",
       "         0.14415549, 0.10207191, 0.23032282, 0.39141769, 0.05919227,\n",
       "         0.32533613, 0.09624185, 0.11684401, 0.10422691, 0.1410753 ,\n",
       "         0.42448593, 0.1605052 , 0.10704206, 0.4337489 , 0.03076092,\n",
       "         0.08614528, 0.34824616, 0.49865305, 0.23122957, 0.12511396,\n",
       "         0.49677184, 0.36844407, 0.76411321, 0.47277205, 0.07121383,\n",
       "         0.04526608, 0.23201999, 0.31332486, 0.19539046, 0.11106275,\n",
       "         0.51171509, 0.04720155, 0.46641754, 0.6075943 , 0.15837365,\n",
       "         0.7011841 , 0.95945631, 0.72227766, 0.76643234, 0.36596248,\n",
       "         0.13757338, 0.57089413, 0.27209541, 0.25204219, 0.63791559,\n",
       "         0.7970432 , 0.080537  , 0.11381265, 0.72668742, 0.36859959,\n",
       "         0.85396426, 0.5673682 , 0.10625212, 0.31945124, 0.0718574 ,\n",
       "         0.01607574, 0.80964408, 0.20810861, 0.31545972, 0.08130382,\n",
       "         0.28529673, 0.16649164, 0.11735796, 0.22385394, 0.65313272,\n",
       "         0.23329659, 0.8698525 , 0.43810569, 0.61474588, 0.04307273,\n",
       "         0.31204595, 0.54323282, 0.10827776, 0.32924856, 0.62688182,\n",
       "         0.26352525, 0.36672114, 0.77248712, 0.66823058, 0.11109896,\n",
       "         0.15082389, 0.08152456, 0.24951709, 0.76178153, 0.18090359,\n",
       "         0.41480821, 0.32381968, 0.77016655, 0.14610633, 0.10202429,\n",
       "         0.90184399, 0.76903676, 0.20929302, 0.17946259, 0.26177413,\n",
       "         0.0652748 , 0.20533254, 0.37904086, 0.36582624, 0.14882436,\n",
       "         0.33987881, 0.20679052, 0.28795454, 0.37984771, 0.08045377,\n",
       "         0.22937812, 0.2282193 , 0.8316562 , 0.11854517, 0.1162649 ,\n",
       "         0.18495803, 0.1226872 , 0.11991063, 0.16536274, 0.22736522,\n",
       "         0.76566308, 0.1684925 , 0.09652961, 0.65950488, 0.92794432,\n",
       "         0.3132476 , 0.69031769, 0.31158577, 0.82430422, 0.57157116,\n",
       "         0.5415227 , 0.27628643, 0.10743857, 0.66526269, 0.72160442,\n",
       "         0.44174953, 0.47935448, 0.31947814, 0.16558767, 0.89963721,\n",
       "         0.09767876, 0.93323626, 0.08903045, 0.32079676, 0.31827591,\n",
       "         0.17251606, 0.28630598, 0.07344453])}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model(x: np.ndarray, y: np.ndarray) -> dict:\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    svm_results = {}\n",
    "    for kernel in kernels:\n",
    "        svm_model = SVC(kernel=kernel, probability=True, max_iter=1000)\n",
    "        scores = cross_val_score(svm_model, x, y, cv=kf, scoring='precision')\n",
    "        svm_results[kernel] = {\n",
    "            'mean': scores.mean(),\n",
    "            'std': scores.std()\n",
    "        }\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(x, y)\n",
    "    lr_scores = cross_val_score(lr_model, x, y, cv=kf, scoring='precision')\n",
    "    lr_predict = lr_model.predict_proba(x)[:, 1]\n",
    "    lr_results = {\n",
    "        'mean': lr_scores.mean(),\n",
    "        'std': lr_scores.std(),\n",
    "        'predictions': lr_predict\n",
    "    }\n",
    "\n",
    "    print(\"SVM Results:\")\n",
    "    for kernel, result in svm_results.items():\n",
    "        print(f\"{kernel}: precision = {result['mean']:.4f} ± {result['std']:.4f}\")\n",
    "    print(\n",
    "        f\"\\nLogistic Regression: precision = {lr_results['mean']:.4f} ± {lr_results['std']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'svm_results': svm_results,\n",
    "        'lr_results': lr_results\n",
    "    }\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "run_model(X.values, y.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
